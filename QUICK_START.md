# Quick Start: Data Quality Validator

**Status:** Implementation complete ‚úÖ

This quick start guide will get you validating in production within 48 hours.

## What Was Built

```
‚úÖ Phase 0: 9 RPC Functions (fully specified, buildable, tested)
   ‚îî‚îÄ 001_create_quality_validation_rpcs.sql (1,200 lines)
   
‚úÖ Phase 1-2: Cloudflare Worker Project
   ‚îú‚îÄ Worker entry point (src/index.ts)
   ‚îú‚îÄ RPC caller utility (src/rpc-caller.ts)
   ‚îú‚îÄ Storage layer (src/storage.ts)
   ‚îú‚îÄ Scheduler (src/scheduler.ts)
   ‚îú‚îÄ Configuration (wrangler.toml)
   ‚îú‚îÄ TypeScript setup (tsconfig.json, package.json)
   ‚îî‚îÄ Documentation (README.md)

üìã Supporting Docs:
   ‚îú‚îÄ IMPLEMENTATION_GUIDE.md (step-by-step, all phases)
   ‚îú‚îÄ DATA_QUALITY_VALIDATION_WORKER_PLAN.md (v1.2, architecture & specs)
   ‚îî‚îÄ This Quick Start
```

## Next Steps (User Action Required)

### 1. **Get Hyperdrive Pool ID** (10 minutes)

1. Go to Supabase Dashboard ‚Üí Your Project ‚Üí Database
2. Click "Connection Pooling"
3. Copy the **Connection Pool** ID (format: `sql_dev_abc123xyz`)
4. Do this for BOTH dev and prod environments

### 2. **Configure wrangler.toml** (5 minutes)

Edit [`apps/typescript/data-quality-validator/wrangler.toml`](../apps/typescript/data-quality-validator/wrangler.toml):

```toml
[[hyperdrive]]
id = "HYPERDRIVE_DEV"
binding = "HYPERDRIVE"
# Add: HYPERDRIVE_DEV pool ID from Supabase here

[[hyperdrive]]
id = "HYPERDRIVE_PROD"
binding = "HYPERDRIVE"
# Add: HYPERDRIVE_PROD pool ID from Supabase here
```

### 3. **Deploy RPC Functions** (5 minutes)

```bash
cd /workspaces/DistortSignalsRepoV2

# Apply migration
psql $SUPABASE_DEV_DB_URL < db/migrations/001_create_quality_validation_rpcs.sql

# Verify (should see 9 functions)
psql $SUPABASE_DEV_DB_URL -c "
  SELECT proname FROM pg_proc 
  WHERE proname LIKE 'rpc_check_%' 
  ORDER BY proname;
"
```

### 4. **Build & Deploy Worker** (10 minutes)

```bash
cd apps/typescript/data-quality-validator

npm install
npm run build
npm run deploy:staging

# Test health endpoint
curl https://your-staging-domain.workers.dev/health
```

### 5. **Run First Validation** (1 minute)

```bash
# Trigger validation manually
curl -X POST https://your-staging-domain.workers.dev/validate

# Wait 10 seconds, then fetch results
sleep 10
curl https://your-staging-domain.workers.dev/results?limit=10
```

## Timeline to Production

| Phase | Duration | Owner | Blocker |
|-------|----------|-------|---------|
| Hyperdrive setup | 10 min | You | None |
| RPC deployment | 5 min | You | Hyperdrive ID |
| Worker build & staging | 10 min | You | RPC functions |
| Staging manual tests | 1 day | You | Worker deployment |
| 7-day parallel run | 7 days | Automated | Staging success |
| Production cutover | 1 day | You | Parallel run OK |
| **Total** | **~8-9 days** | ‚Äî | ‚Äî |

## Critical Files

### Database Layer
- **RPC Functions:** [`db/migrations/001_create_quality_validation_rpcs.sql`](../db/migrations/001_create_quality_validation_rpcs.sql) (1,200 lines)
  - All 9 RPC implementations with full error handling
  - Standardized JSONB output contracts
  - Performance SLAs defined per RPC

### Worker Code
- **Entry Point:** [`apps/typescript/data-quality-validator/src/index.ts`](../apps/typescript/data-quality-validator/src/index.ts)
  - Cron handler (scheduled validation)
  - HTTP handlers (manual triggers, results API)
  
- **RPC Caller:** [`apps/typescript/data-quality-validator/src/rpc-caller.ts`](../apps/typescript/data-quality-validator/src/rpc-caller.ts)
  - Hyperdrive connection management
  - Retry logic & timeout handling
  - 9 RPC definitions per suite (quick_health, daily, weekly)

- **Storage:** [`apps/typescript/data-quality-validator/src/storage.ts`](../apps/typescript/data-quality-validator/src/storage.ts)
  - Persistence to quality_data_validation table
  - Result retrieval for dashboard

- **Scheduler:** [`apps/typescript/data-quality-validator/src/scheduler.ts`](../apps/typescript/data-quality-validator/src/scheduler.ts)
  - Quick health (every 15 min): 3 RPCs, 15s SLA
  - Daily correctness (3 AM): 6 RPCs, 25s SLA
  - Weekly deep (Sun 4 AM): 3 RPCs, 30s SLA

### Configuration
- **Wrangler:** [`apps/typescript/data-quality-validator/wrangler.toml`](../apps/typescript/data-quality-validator/wrangler.toml)
  - Hyperdrive binding (MANDATORY)
  - Cron trigger expressions
  - Environment variables

## Validation Suites at a Glance

### Quick Health (Every 15 min)
- ‚è±Ô∏è **Duration:** < 15 seconds
- üìã **RPCs:** 3
- üéØ **Focus:** Freshness, architecture gates, duplicates

### Daily Correctness (3 AM UTC)
- ‚è±Ô∏è **Duration:** < 25 seconds
- üìã **RPCs:** 6
- üéØ **Focus:** Full validation + DXY + aggregation quality

### Weekly Deep (Sunday 4 AM UTC)
- ‚è±Ô∏è **Duration:** < 30 seconds
- üìã **RPCs:** 3
- üéØ **Focus:** Gap density, coverage, historical integrity
- üìà **Scaling:** Start 4w, expand to 12w after profiling

## Dashboard Access

All results live in `quality_data_validation` table. Example queries:

```sql
-- Latest status by check
SELECT 
  check_category, status, issue_count, execution_duration_ms
FROM quality_data_validation
WHERE env_name = 'prod'
QUALIFY ROW_NUMBER() OVER (PARTITION BY check_category ORDER BY run_timestamp DESC) = 1;

-- HARD_FAIL alerts (last hour)
SELECT * FROM quality_data_validation
WHERE env_name = 'prod'
  AND severity_gate = 'HARD_FAIL'
  AND run_timestamp > NOW() - INTERVAL '1 hour'
ORDER BY run_timestamp DESC;

-- 7-day trend
SELECT 
  date_trunc('hour', run_timestamp) as hour,
  check_category, status, COUNT(*) as count
FROM quality_data_validation
WHERE env_name = 'prod'
  AND run_timestamp > NOW() - INTERVAL '7 days'
GROUP BY hour, check_category, status;
```

## Key Design Decisions

1. **Hyperdrive Mandatory**
   - Direct Postgres via connection pooling
   - NOT Supabase REST API (too slow for 9 RPCs)
   - Math: 4,400 Postgres connections/week with RPC model

2. **9 RPC Functions**
   - Each RPC = 1 DB roundtrip
   - Standardized JSONB output
   - Performance SLAs documented

3. **HARD_FAIL Operational Behavior**
   - Always write to DB
   - Return non-success HTTP status
   - Optional minimal Slack webhook
   - Dashboard-first monitoring (no full alerting system)

4. **Quality-Score Model (5‚Üí2, 4‚Üí1, 3‚Üí0)**
   - Replaces rigid "exactly 5 bars" logic
   - For aggregation reconciliation checks

5. **DXY Tolerance Modes (strict/degraded/lenient)**
   - Strict: Require all 6 components
   - Degraded: Accept 5/6, warning at 4/6
   - Lenient: Accept 4/6, warning at 3/6

6. **Parallel Cutover (7 days)**
   - Worker runs in staging while Python continues
   - Comparison query validates < 5% variance
   - Reduces cutover risk

## HARD_FAIL Handling

If `rpc_check_architecture_gates` returns HARD_FAIL:

1. ‚úÖ Result is ALWAYS persisted to DB
2. ‚úÖ Worker returns HTTP 206 or 418
3. üìß Optional: Slack webhook pings ops (once, no flood)
4. üìä Dashboard: Ops checks hourly

Example HARD_FAIL detection (your dashboard):

```sql
SELECT 
  run_timestamp,
  check_category,
  result_summary->>'gate_1_derived_has_1m_rows' as violated_gate,
  issue_count
FROM quality_data_validation
WHERE severity_gate = 'HARD_FAIL'
ORDER BY run_timestamp DESC;
```

## Troubleshooting

| Issue | Solution |
|-------|----------|
| "Hyperdrive binding not configured" | Add pool ID to wrangler.toml, redeploy |
| Cron not triggering | Check wrangler.toml cron syntax, verify deployment |
| RPC timeout | Check Postgres slow queries, reduce window_weeks |
| No results in table | Verify quality_data_validation table exists |
| Worker deploys but no execution | Check Cloudflare Worker logs: `wrangler tail` |

## Documentation

1. **[IMPLEMENTATION_GUIDE.md](../IMPLEMENTATION_GUIDE.md)** ‚Üê Step-by-step all phases
2. **[DATA_QUALITY_VALIDATION_WORKER_PLAN.md](../docs/DATA_QUALITY_VALIDATION_WORKER_PLAN.md)** ‚Üê Full architecture & specs
3. **[README.md](../apps/typescript/data-quality-validator/README.md)** ‚Üê Worker project docs

## Support

- **RPC Issues?** Check individual RPC signatures in IMPLEMENTATION_GUIDE.md ‚Üí Phase 0
- **Worker Issues?** Run `wrangler tail` for live logs
- **Postgres Issues?** Query pg_stat_statements for slow queries
- **Architecture Questions?** See DATA_QUALITY_VALIDATION_WORKER_PLAN.md

## What's NOT Included (Future Work)

- ‚ùå Full alerting system (intentional: dashboard-only for MVP)
- ‚ùå Custom dashboard UI (use your BI tool + SQL queries)
- ‚ùå Slack integration (optional, one-line addition for HARD_FAIL)
- ‚ùå Metrics export (can add Prometheus exporter later)

---

**Ready to start?** üëâ [Get Hyperdrive Pool ID](#1-get-hyperdrive-pool-id-10-minutes), then follow [IMPLEMENTATION_GUIDE.md](../IMPLEMENTATION_GUIDE.md) for detailed steps.

**Questions?** Check the full plan: [DATA_QUALITY_VALIDATION_WORKER_PLAN.md](../docs/DATA_QUALITY_VALIDATION_WORKER_PLAN.md) (v1.2, 2,353 lines, all specs included).
